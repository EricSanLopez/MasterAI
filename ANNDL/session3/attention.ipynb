{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i1KfYGFRRWP"
      },
      "source": [
        "## Artificial Neural Networks and Deep Learning  \n",
        "##Assignment 3.3 - Self-attention and Transformers\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we first understand the self-attention mechanism by implementing it both with ``NumPy`` and ``PyTorch``.\n",
        "Then, we implement a 6-layer Vision Transformer (ViT) and train it on the MNIST dataset.\n",
        "\n",
        "All training will be conducted on a single T4 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlTJbgaaRTct"
      },
      "outputs": [],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qu6w5GLkRezN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0daf73db-927d-4cf6-fb14-838b783f2248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  5 14:32:12 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-sUz1A9SzVH"
      },
      "source": [
        "# Self-attention Mechanism\n",
        "Self-attention is the core mechanism in Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6ol1XZtiPpk"
      },
      "source": [
        "## Self-attention with NumPy\n",
        "To have a better understanding of it, we first manually implement self-attention mechanism with ``numpy``. You can check the dimension of each variable during the matrix computation.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AgWIgp51RgC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916d6b5e-2b62-4a5f-d7d9-74f962915cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The attention outputs are\n",
            " [[-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]\n",
            " [-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]\n",
            " [-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]\n",
            " ...\n",
            " [-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]\n",
            " [-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]\n",
            " [-1.61863     2.7844908  -0.91642446 ...  2.18249428  2.47293919\n",
            "  -1.82993738]]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "\n",
        "# I. Define the input data X\n",
        "# X consists out of 32 samples, each sample has dimensionality 256\n",
        "n = 32\n",
        "d = 256\n",
        "X = randn(n, d) # (32, 256)\n",
        "\n",
        "# II. Generate the projection weights\n",
        "Wq = randn(d, d) #(256, 256)\n",
        "Wk = randn(d, d)\n",
        "Wv = randn(d, d)\n",
        "\n",
        "# III. Project X to find its query, keys and values vectors\n",
        "Q = np.dot(X, Wq) # (32, 256)\n",
        "K = np.dot(X, Wk)\n",
        "V = np.dot(X, Wv)\n",
        "\n",
        "# IV. Compute the self-attention score, denoted by A\n",
        "# A = softmax(QK^T / \\sqrt{d})\n",
        "# Define the softmax function\n",
        "def softmax(z):\n",
        "    z = np.clip(z, 100, -100) # clip in case softmax explodes\n",
        "    tmp = np.exp(z)\n",
        "    res = np.exp(z) / np.sum(tmp, axis=1)\n",
        "    return res\n",
        "\n",
        "A = softmax(np.dot(Q, K.transpose())/math.sqrt(d)) #(32, 32)\n",
        "\n",
        "# V. Compute the self-attention output\n",
        "# outputs = A * V\n",
        "outputs = np.dot(A, V) #(32, 256)\n",
        "\n",
        "print(\"The attention outputs are\\n {}\".format(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iozM1k4khO0B"
      },
      "source": [
        "## Self-attention with PyTorch\n",
        "Now, we implement self-attention with ``PyTorch``, which is commonly used when building Transformers.\n",
        "\n",
        "Feel free to change the dimensions of each variable and see how the output dimension will change accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qng07v8xdaPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12d8171-1f84-4f08-dc43-fa977c0da800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape:torch.Size([32, 20, 128]) \n",
            " Q.shape:torch.Size([32, 20, 64]) \n",
            " K.shape:torch.Size([32, 20, 64]) \n",
            " V.shape:torch.Size([32, 20, 32])\n",
            "attention matrix:  torch.Size([32, 20, 20])\n",
            "attention outputs:  torch.Size([32, 20, 32])\n",
            "tensor([[[-0.2012,  0.0361, -0.2771,  ...,  0.0593,  0.1849, -0.2348],\n",
            "         [-0.1910,  0.0571, -0.2515,  ...,  0.0931,  0.1622, -0.2577],\n",
            "         [-0.0693,  0.0872, -0.2442,  ...,  0.0319,  0.2204, -0.2084],\n",
            "         ...,\n",
            "         [-0.1353,  0.0258, -0.2646,  ...,  0.0674,  0.1297, -0.2350],\n",
            "         [-0.1040,  0.1042, -0.3023,  ...,  0.0234,  0.0933, -0.3487],\n",
            "         [-0.1450,  0.0278, -0.2788,  ...,  0.0490,  0.1309, -0.1918]],\n",
            "\n",
            "        [[ 0.1055,  0.0957,  0.0789,  ..., -0.0527,  0.1576,  0.1550],\n",
            "         [ 0.1538,  0.0214, -0.0481,  ...,  0.0121,  0.0562,  0.0941],\n",
            "         [ 0.0642,  0.1360, -0.0200,  ..., -0.0777,  0.1175,  0.1415],\n",
            "         ...,\n",
            "         [ 0.0991,  0.0681, -0.0440,  ..., -0.0869,  0.0892,  0.1008],\n",
            "         [ 0.0012,  0.0357, -0.1250,  ..., -0.0616,  0.1268,  0.0826],\n",
            "         [ 0.1062,  0.0770, -0.0067,  ..., -0.0571,  0.0896,  0.1111]],\n",
            "\n",
            "        [[ 0.0383, -0.0458,  0.0172,  ..., -0.0244,  0.4817,  0.1963],\n",
            "         [ 0.0343,  0.0073,  0.0941,  ..., -0.0721,  0.3676,  0.1409],\n",
            "         [ 0.0123,  0.0868,  0.0738,  ...,  0.0485,  0.4572,  0.1352],\n",
            "         ...,\n",
            "         [-0.0009,  0.0461,  0.0710,  ...,  0.0585,  0.3796,  0.0566],\n",
            "         [ 0.0158, -0.0138,  0.0269,  ..., -0.0407,  0.2669,  0.0863],\n",
            "         [-0.0612,  0.0160,  0.0517,  ...,  0.1412,  0.2508,  0.1047]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0674,  0.0218,  0.1889,  ..., -0.0764, -0.0223,  0.3919],\n",
            "         [ 0.1361, -0.0586,  0.0830,  ..., -0.0688, -0.0185,  0.4851],\n",
            "         [ 0.0010,  0.0221,  0.0300,  ..., -0.1103, -0.0723,  0.3787],\n",
            "         ...,\n",
            "         [ 0.0270,  0.0328,  0.0169,  ..., -0.1336, -0.0553,  0.3699],\n",
            "         [ 0.0488, -0.0499,  0.1144,  ..., -0.0725, -0.0334,  0.3023],\n",
            "         [ 0.0345,  0.0508,  0.0975,  ..., -0.1313, -0.0475,  0.3660]],\n",
            "\n",
            "        [[-0.1414,  0.2370,  0.2239,  ..., -0.0299,  0.1437,  0.1599],\n",
            "         [ 0.0049,  0.1657,  0.1612,  ...,  0.0555,  0.1139,  0.2350],\n",
            "         [-0.0620,  0.1322,  0.2849,  ...,  0.0393,  0.1802,  0.1199],\n",
            "         ...,\n",
            "         [-0.0237,  0.0953,  0.2937,  ...,  0.0500,  0.2755,  0.1241],\n",
            "         [-0.0702,  0.1973,  0.2971,  ...,  0.0388,  0.2209,  0.2644],\n",
            "         [-0.0284,  0.1042,  0.2113,  ..., -0.0361,  0.1803,  0.1198]],\n",
            "\n",
            "        [[-0.0573,  0.2435,  0.0236,  ..., -0.1587, -0.0326, -0.0057],\n",
            "         [-0.1136,  0.2125,  0.0591,  ..., -0.1812, -0.0280,  0.0383],\n",
            "         [ 0.0218,  0.2215,  0.0823,  ..., -0.2051, -0.0403,  0.1159],\n",
            "         ...,\n",
            "         [ 0.0025,  0.2247,  0.0184,  ..., -0.2338,  0.0615,  0.0497],\n",
            "         [-0.0934,  0.1767,  0.0130,  ..., -0.2534,  0.0584, -0.0385],\n",
            "         [-0.0323,  0.2646,  0.0571,  ..., -0.1811, -0.0568, -0.0089]]],\n",
            "       grad_fn=<BmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_input, dim_q, dim_v):\n",
        "        '''\n",
        "        dim_input: the dimension of each sample\n",
        "        dim_q: dimension of Q matrix, should be equal to dim_k\n",
        "        dim_v: dimension of V matrix, also the  dimension of the attention output\n",
        "        '''\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_q = dim_q\n",
        "        self.dim_k = dim_q\n",
        "        self.dim_v = dim_v\n",
        "\n",
        "        # Define the linear projection\n",
        "        self.linear_q = nn.Linear(self.dim_input, self.dim_q, bias=False)\n",
        "        self.linear_k = nn.Linear(self.dim_input, self.dim_k, bias=False)\n",
        "        self.linear_v = nn.Linear(self.dim_input, self.dim_v, bias=False)\n",
        "        self._norm_fact = 1 / math.sqrt(self.dim_k)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, n, dim_q = x.shape\n",
        "\n",
        "        q = self.linear_q(x) # (batchsize, seq_len, dim_q)\n",
        "        k = self.linear_k(x) # (batchsize, seq_len, dim_k)\n",
        "        v = self.linear_v(x) # (batchsize, seq_len, dim_v)\n",
        "        print(f'x.shape:{x.shape} \\n Q.shape:{q.shape} \\n K.shape:{k.shape} \\n V.shape:{v.shape}')\n",
        "\n",
        "        dist = torch.bmm(q, k.transpose(1,2)) * self._norm_fact\n",
        "        dist = torch.softmax(dist, dim=-1)\n",
        "        print('attention matrix: ', dist.shape)\n",
        "\n",
        "        outputs = torch.bmm(dist, v)\n",
        "        print('attention outputs: ', outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "batch_size = 32 # number of samples in a batch\n",
        "dim_input = 128 # dimension of each item in the sample sequence\n",
        "seq_len = 20 # sequence length for each sample\n",
        "x = torch.randn(batch_size, seq_len, dim_input)\n",
        "self_attention = SelfAttention(dim_input, dim_q = 64, dim_v = 32)\n",
        "\n",
        "attention = self_attention(x)\n",
        "\n",
        "print(attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZaAFL8MS2Ng"
      },
      "source": [
        "# Transformers\n",
        "In this section, we implement a 6-layer Vision Transformer (ViT) and trained it on the MNIST dataset.\n",
        "We consider the classification tasks.\n",
        "First, we load the MNIST dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rZ-eIaeZjWjL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "def get_mnist_loader(batch_size=100, shuffle=True):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=shuffle)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=False)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C06IoPIjePg",
        "outputId": "288d1cc6-bab9-4986-86d7-d2db43f885c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ],
      "source": [
        "# This package is needed to build the transformer\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx9eZrMpmA2z"
      },
      "source": [
        "## Build ViT from scratch\n",
        "Recall that each Transformer block include 2 modules: the self-attention module, the feedforward module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vr6d7IWfjpxY"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=8):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n",
        "\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n",
        "            mask = mask[:, None, :] * mask[:, :, None]\n",
        "            dots.masked_fill_(~mask, float('-inf'))\n",
        "            del mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n",
        "                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, mask=mask)\n",
        "            x = ff(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(), # Gaussian Error Linear Units is another type of activation function\n",
        "            nn.Linear(mlp_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img, mask=None):\n",
        "        p = self.patch_size\n",
        "\n",
        "        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
        "        x = self.patch_to_embedding(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding\n",
        "        x = self.transformer(x, mask)\n",
        "\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTawNC64mhBO"
      },
      "source": [
        "## Training and test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rKJ4tjCjjycH"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vph2CrNxj6ZZ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # We do not need to remember the gradients when testing\n",
        "    # This will help reduce memory\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRYys50km0-E"
      },
      "source": [
        "## Let's start training!\n",
        "Here, you can change the ViT structure by changing the hyper-parametrs inside ``ViT`` function.\n",
        "The default settings are with 6 layers, 8 heads for the multi-head attention mechanism and embedding dimension of 64.\n",
        "You can also increase the number of epochs to obtain better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rVLJLLDuj7yQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# You can change the architecture here\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.cuda()\n",
        "# We also print the network architecture\n",
        "model\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vlt3tk-MkDB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b6e037-62fa-4e24-d531-e0cedc9f99c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.55MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.42MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 LR: [0.001]\n",
            "[    0/60000 (  0%)]  Loss: 2.3259\n",
            "[12800/60000 ( 21%)]  Loss: 1.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.3386\n",
            "[38400/60000 ( 64%)]  Loss: 0.2360\n",
            "[51200/60000 ( 85%)]  Loss: 0.1861\n",
            "\n",
            "Average test loss: 0.1779  Accuracy: 9427/10000 (94.27%)\n",
            "\n",
            "Epoch: 2 LR: [0.00095]\n",
            "[    0/60000 (  0%)]  Loss: 0.0850\n",
            "[12800/60000 ( 21%)]  Loss: 0.1713\n",
            "[25600/60000 ( 43%)]  Loss: 0.0594\n",
            "[38400/60000 ( 64%)]  Loss: 0.1821\n",
            "[51200/60000 ( 85%)]  Loss: 0.1840\n",
            "\n",
            "Average test loss: 0.1370  Accuracy: 9566/10000 (95.66%)\n",
            "\n",
            "Epoch: 3 LR: [0.0009025]\n",
            "[    0/60000 (  0%)]  Loss: 0.1887\n",
            "[12800/60000 ( 21%)]  Loss: 0.0165\n",
            "[25600/60000 ( 43%)]  Loss: 0.0711\n",
            "[38400/60000 ( 64%)]  Loss: 0.0727\n",
            "[51200/60000 ( 85%)]  Loss: 0.0744\n",
            "\n",
            "Average test loss: 0.0898  Accuracy: 9719/10000 (97.19%)\n",
            "\n",
            "Epoch: 4 LR: [0.000857375]\n",
            "[    0/60000 (  0%)]  Loss: 0.0354\n",
            "[12800/60000 ( 21%)]  Loss: 0.0284\n",
            "[25600/60000 ( 43%)]  Loss: 0.0210\n",
            "[38400/60000 ( 64%)]  Loss: 0.0799\n",
            "[51200/60000 ( 85%)]  Loss: 0.0781\n",
            "\n",
            "Average test loss: 0.0839  Accuracy: 9761/10000 (97.61%)\n",
            "\n",
            "Epoch: 5 LR: [0.0008145062499999999]\n",
            "[    0/60000 (  0%)]  Loss: 0.0890\n",
            "[12800/60000 ( 21%)]  Loss: 0.0442\n",
            "[25600/60000 ( 43%)]  Loss: 0.0334\n",
            "[38400/60000 ( 64%)]  Loss: 0.0455\n",
            "[51200/60000 ( 85%)]  Loss: 0.0171\n",
            "\n",
            "Average test loss: 0.0725  Accuracy: 9765/10000 (97.65%)\n",
            "\n",
            "Epoch: 6 LR: [0.0007737809374999998]\n",
            "[    0/60000 (  0%)]  Loss: 0.0428\n",
            "[12800/60000 ( 21%)]  Loss: 0.0642\n",
            "[25600/60000 ( 43%)]  Loss: 0.0275\n",
            "[38400/60000 ( 64%)]  Loss: 0.0542\n",
            "[51200/60000 ( 85%)]  Loss: 0.0921\n",
            "\n",
            "Average test loss: 0.0941  Accuracy: 9734/10000 (97.34%)\n",
            "\n",
            "Epoch: 7 LR: [0.0007350918906249997]\n",
            "[    0/60000 (  0%)]  Loss: 0.1009\n",
            "[12800/60000 ( 21%)]  Loss: 0.0366\n",
            "[25600/60000 ( 43%)]  Loss: 0.0124\n",
            "[38400/60000 ( 64%)]  Loss: 0.0077\n",
            "[51200/60000 ( 85%)]  Loss: 0.0164\n",
            "\n",
            "Average test loss: 0.0856  Accuracy: 9736/10000 (97.36%)\n",
            "\n",
            "Epoch: 8 LR: [0.0006983372960937497]\n",
            "[    0/60000 (  0%)]  Loss: 0.1259\n",
            "[12800/60000 ( 21%)]  Loss: 0.0145\n",
            "[25600/60000 ( 43%)]  Loss: 0.0195\n",
            "[38400/60000 ( 64%)]  Loss: 0.1008\n",
            "[51200/60000 ( 85%)]  Loss: 0.0911\n",
            "\n",
            "Average test loss: 0.0750  Accuracy: 9761/10000 (97.61%)\n",
            "\n",
            "Epoch: 9 LR: [0.0006634204312890621]\n",
            "[    0/60000 (  0%)]  Loss: 0.0345\n",
            "[12800/60000 ( 21%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0218\n",
            "[38400/60000 ( 64%)]  Loss: 0.0189\n",
            "[51200/60000 ( 85%)]  Loss: 0.0103\n",
            "\n",
            "Average test loss: 0.0592  Accuracy: 9830/10000 (98.30%)\n",
            "\n",
            "Epoch: 10 LR: [0.000630249409724609]\n",
            "[    0/60000 (  0%)]  Loss: 0.0109\n",
            "[12800/60000 ( 21%)]  Loss: 0.0111\n",
            "[25600/60000 ( 43%)]  Loss: 0.0279\n",
            "[38400/60000 ( 64%)]  Loss: 0.0072\n",
            "[51200/60000 ( 85%)]  Loss: 0.0121\n",
            "\n",
            "Average test loss: 0.0711  Accuracy: 9778/10000 (97.78%)\n",
            "\n",
            "Epoch: 11 LR: [0.0005987369392383785]\n",
            "[    0/60000 (  0%)]  Loss: 0.0235\n",
            "[12800/60000 ( 21%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0164\n",
            "[38400/60000 ( 64%)]  Loss: 0.0477\n",
            "[51200/60000 ( 85%)]  Loss: 0.0048\n",
            "\n",
            "Average test loss: 0.0655  Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch: 12 LR: [0.0005688000922764595]\n",
            "[    0/60000 (  0%)]  Loss: 0.0067\n",
            "[12800/60000 ( 21%)]  Loss: 0.0217\n",
            "[25600/60000 ( 43%)]  Loss: 0.0214\n",
            "[38400/60000 ( 64%)]  Loss: 0.0102\n",
            "[51200/60000 ( 85%)]  Loss: 0.0016\n",
            "\n",
            "Average test loss: 0.0671  Accuracy: 9818/10000 (98.18%)\n",
            "\n",
            "Epoch: 13 LR: [0.0005403600876626365]\n",
            "[    0/60000 (  0%)]  Loss: 0.0079\n",
            "[12800/60000 ( 21%)]  Loss: 0.0017\n",
            "[25600/60000 ( 43%)]  Loss: 0.0275\n",
            "[38400/60000 ( 64%)]  Loss: 0.0164\n",
            "[51200/60000 ( 85%)]  Loss: 0.0116\n",
            "\n",
            "Average test loss: 0.0587  Accuracy: 9846/10000 (98.46%)\n",
            "\n",
            "Epoch: 14 LR: [0.0005133420832795047]\n",
            "[    0/60000 (  0%)]  Loss: 0.0096\n",
            "[12800/60000 ( 21%)]  Loss: 0.0500\n",
            "[25600/60000 ( 43%)]  Loss: 0.0127\n",
            "[38400/60000 ( 64%)]  Loss: 0.0212\n",
            "[51200/60000 ( 85%)]  Loss: 0.0181\n",
            "\n",
            "Average test loss: 0.0652  Accuracy: 9830/10000 (98.30%)\n",
            "\n",
            "Epoch: 15 LR: [0.00048767497911552944]\n",
            "[    0/60000 (  0%)]  Loss: 0.0063\n",
            "[12800/60000 ( 21%)]  Loss: 0.0269\n",
            "[25600/60000 ( 43%)]  Loss: 0.0032\n",
            "[38400/60000 ( 64%)]  Loss: 0.0010\n",
            "[51200/60000 ( 85%)]  Loss: 0.0121\n",
            "\n",
            "Average test loss: 0.0613  Accuracy: 9849/10000 (98.49%)\n",
            "\n",
            "Epoch: 16 LR: [0.00046329123015975297]\n",
            "[    0/60000 (  0%)]  Loss: 0.0387\n",
            "[12800/60000 ( 21%)]  Loss: 0.0150\n",
            "[25600/60000 ( 43%)]  Loss: 0.0069\n",
            "[38400/60000 ( 64%)]  Loss: 0.0154\n",
            "[51200/60000 ( 85%)]  Loss: 0.0064\n",
            "\n",
            "Average test loss: 0.0665  Accuracy: 9839/10000 (98.39%)\n",
            "\n",
            "Epoch: 17 LR: [0.0004401266686517653]\n",
            "[    0/60000 (  0%)]  Loss: 0.0018\n",
            "[12800/60000 ( 21%)]  Loss: 0.0031\n",
            "[25600/60000 ( 43%)]  Loss: 0.0084\n",
            "[38400/60000 ( 64%)]  Loss: 0.0029\n",
            "[51200/60000 ( 85%)]  Loss: 0.0378\n",
            "\n",
            "Average test loss: 0.0790  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 18 LR: [0.00041812033521917703]\n",
            "[    0/60000 (  0%)]  Loss: 0.0170\n",
            "[12800/60000 ( 21%)]  Loss: 0.0034\n",
            "[25600/60000 ( 43%)]  Loss: 0.0022\n",
            "[38400/60000 ( 64%)]  Loss: 0.0036\n",
            "[51200/60000 ( 85%)]  Loss: 0.0050\n",
            "\n",
            "Average test loss: 0.0639  Accuracy: 9848/10000 (98.48%)\n",
            "\n",
            "Epoch: 19 LR: [0.00039721431845821814]\n",
            "[    0/60000 (  0%)]  Loss: 0.0019\n",
            "[12800/60000 ( 21%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0015\n",
            "[38400/60000 ( 64%)]  Loss: 0.0405\n",
            "[51200/60000 ( 85%)]  Loss: 0.0019\n",
            "\n",
            "Average test loss: 0.0661  Accuracy: 9853/10000 (98.53%)\n",
            "\n",
            "Epoch: 20 LR: [0.0003773536025353072]\n",
            "[    0/60000 (  0%)]  Loss: 0.0028\n",
            "[12800/60000 ( 21%)]  Loss: 0.0049\n",
            "[25600/60000 ( 43%)]  Loss: 0.0008\n",
            "[38400/60000 ( 64%)]  Loss: 0.0010\n",
            "[51200/60000 ( 85%)]  Loss: 0.0034\n",
            "\n",
            "Average test loss: 0.0700  Accuracy: 9843/10000 (98.43%)\n",
            "\n",
            "Execution time: 277.57 seconds\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "# Gradually reduce the learning rate while training\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "    scheduler.step()\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XicoRf8_nUTK"
      },
      "outputs": [],
      "source": [
        "skip=True\n",
        "for dim in [32, 64, 128, 256]:\n",
        "    for depth in [4, 6, 12]:\n",
        "        for heads in [4, 8, 16]:\n",
        "            for mlp_dim in [128, 256, 512]:\n",
        "                if dim == 64 and depth == 12 and heads == 4 and mlp_dim == 512: # progress\n",
        "                    skip = False\n",
        "                if skip:\n",
        "                    continue\n",
        "                print(\"dim\", dim, \"depth\", depth, \"heads\", heads,\n",
        "                      \"mlp_dim\", mlp_dim)\n",
        "\n",
        "                # You can change the architecture here\n",
        "                model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "                            dim=dim, depth=depth, heads=heads, mlp_dim=mlp_dim)\n",
        "                model = model.cuda()\n",
        "                # We also print the network architecture\n",
        "                model\n",
        "\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "                train_loss_history, test_loss_history = [], []\n",
        "\n",
        "                N_EPOCHS = 20\n",
        "\n",
        "                train_loader, test_loader = get_mnist_loader(batch_size=128, shuffle=True)\n",
        "\n",
        "                # Gradually reduce the learning rate while training\n",
        "                scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "                start_time = time.time()\n",
        "                for epoch in range(1, N_EPOCHS + 1):\n",
        "                    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n",
        "                    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "                    evaluate(model, test_loader, test_loss_history)\n",
        "                    scheduler.step()\n",
        "\n",
        "                print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vVtBMZiB5LnH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}